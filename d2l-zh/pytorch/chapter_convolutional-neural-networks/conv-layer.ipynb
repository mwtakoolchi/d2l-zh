{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d202936b",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Convolutions for Images\n",
    ":label:`sec_conv_layer`\n",
    "\n",
    "Now that we understand how convolutional layers work in theory,\n",
    "we are ready to see how they work in practice.\n",
    "Building on our motivation of convolutional neural networks\n",
    "as efficient architectures for exploring structure in image data,\n",
    "we stick with images as our running example.\n",
    "\n",
    "# 图像卷积\n",
    ":label:`sec_conv_layer`\n",
    "\n",
    "现在我们从理论层面理解了卷积层的工作原理，接下来看看它们在实践中的应用。基于卷积神经网络作为探索图像数据结构的高效架构这一动机，我们将继续以图像作为主要示例进行探讨。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889e59c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:36.399216Z",
     "iopub.status.busy": "2023-08-18T07:02:36.398695Z",
     "iopub.status.idle": "2023-08-18T07:02:39.260516Z",
     "shell.execute_reply": "2023-08-18T07:02:39.259628Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6762e74",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## The Cross-Correlation Operation\n",
    "\n",
    "Recall that strictly speaking, convolutional layers\n",
    "are a  misnomer, since the operations they express\n",
    "are more accurately described as cross-correlations.\n",
    "Based on our descriptions of convolutional layers in :numref:`sec_why-conv`,\n",
    "in such a layer, an input tensor\n",
    "and a kernel tensor are combined\n",
    "to produce an output tensor through a (**cross-correlation operation.**)\n",
    "\n",
    "Let's ignore channels for now and see how this works\n",
    "with two-dimensional data and hidden representations.\n",
    "In :numref:`fig_correlation`,\n",
    "the input is a two-dimensional tensor\n",
    "with a height of 3 and width of 3.\n",
    "We mark the shape of the tensor as $3 \\times 3$ or ($3$, $3$).\n",
    "The height and width of the kernel are both 2.\n",
    "The shape of the *kernel window* (or *convolution window*)\n",
    "is given by the height and width of the kernel\n",
    "(here it is $2 \\times 2$).\n",
    "\n",
    "![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+1\\times1+3\\times2+4\\times3=19$.](../img/correlation.svg)\n",
    ":label:`fig_correlation`\n",
    "\n",
    "In the two-dimensional cross-correlation operation,\n",
    "we begin with the convolution window positioned\n",
    "at the upper-left corner of the input tensor\n",
    "and slide it across the input tensor,\n",
    "both from left to right and top to bottom.\n",
    "When the convolution window slides to a certain position,\n",
    "the input subtensor contained in that window\n",
    "and the kernel tensor are multiplied elementwise\n",
    "and the resulting tensor is summed up\n",
    "yielding a single scalar value.\n",
    "This result gives the value of the output tensor\n",
    "at the corresponding location.\n",
    "Here, the output tensor has a height of 2 and width of 2\n",
    "and the four elements are derived from\n",
    "the two-dimensional cross-correlation operation:\n",
    "\n",
    "$$\n",
    "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
    "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
    "$$\n",
    "\n",
    "Note that along each axis, the output size\n",
    "is slightly smaller than the input size.\n",
    "Because the kernel has width and height greater than $1$,\n",
    "we can only properly compute the cross-correlation\n",
    "for locations where the kernel fits wholly within the image,\n",
    "the output size is given by the input size $n_\\textrm{h} \\times n_\\textrm{w}$\n",
    "minus the size of the convolution kernel $k_\\textrm{h} \\times k_\\textrm{w}$\n",
    "via\n",
    "\n",
    "$$(n_\\textrm{h}-k_\\textrm{h}+1) \\times (n_\\textrm{w}-k_\\textrm{w}+1).$$\n",
    "\n",
    "This is the case since we need enough space\n",
    "to \"shift\" the convolution kernel across the image.\n",
    "Later we will see how to keep the size unchanged\n",
    "by padding the image with zeros around its boundary\n",
    "so that there is enough space to shift the kernel.\n",
    "Next, we implement this process in the `corr2d` function,\n",
    "which accepts an input tensor `X` and a kernel tensor `K`\n",
    "and returns an output tensor `Y`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0c253",
   "metadata": {},
   "source": [
    "## 互相关运算\n",
    "\n",
    "严格来说，卷积层所表达的运算应更准确地称为互相关运算。根据我们在 :numref:`sec_why-conv` 中对卷积层的描述，在该层中，输入张量和核张量通过（**互相关运算**）结合生成输出张量。\n",
    "\n",
    "我们暂时忽略通道维度，先观察二维数据和隐藏表示的情况。如 :numref:`fig_correlation` 所示，输入是一个高度为3、宽度为3的二维张量，其形状记为$3 \\times 3$或（$3$, $3$）。核的高度和宽度都为2，卷积窗口（或称核窗口）的形状由核的高度和宽度决定（此处为$2 \\times 2$）。\n",
    "\n",
    "![二维互相关运算。阴影部分为第一个输出元素，以及用于计算该输出的输入和核张量元素：$0×0+1×1+3×2+4×3=19$。](../img/correlation.svg)\n",
    ":label:`fig_correlation`\n",
    "\n",
    "在二维互相关运算中：\n",
    "1. 卷积窗口初始位于输入张量左上角\n",
    "2. 从左到右、从上到下进行滑动\n",
    "3. 当窗口滑动到指定位置时，窗口内输入子张量与核张量进行逐元素相乘并求和\n",
    "4. 计算结果作为输出张量对应位置的标量值\n",
    "\n",
    "示例中输出张量形状为$2 \\times 2$，四个元素值由以下二维互相关运算得出：\n",
    "\n",
    "$$\n",
    "0×0+1×1+3×2+4×3=19,\\\\\n",
    "1×0+2×1+4×2+5×3=25,\\\\\n",
    "3×0+4×1+6×2+7×3=37,\\\\\n",
    "4×0+5×1+7×2+8×3=43.\n",
    "$$\n",
    "\n",
    "值得注意的是：\n",
    "- 输出尺寸沿每个轴向都会略小于输入尺寸\n",
    "- 当核的宽高大于$1$时，只能在核完全包含在图像内的位置进行有效计算\n",
    "- 输出尺寸公式为：$$(n_\\text{h}-k_\\text{h}+1) × (n_\\text{w}-k_\\text{w}+1)$$\n",
    "\n",
    "后续我们将学习通过零填充（padding）来保持特征图尺寸不变的方法。现在，我们通过实现`corr2d`函数来验证上述过程，该函数接受输入张量`X`和核张量`K`，返回输出张量`Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713836f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.264636Z",
     "iopub.status.busy": "2023-08-18T07:02:39.263963Z",
     "iopub.status.idle": "2023-08-18T07:02:39.269692Z",
     "shell.execute_reply": "2023-08-18T07:02:39.268875Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30ed41",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "We can construct the input tensor `X` and the kernel tensor `K`\n",
    "from :numref:`fig_correlation`\n",
    "to [**validate the output of the above implementation**]\n",
    "of the two-dimensional cross-correlation operation.\n",
    "\n",
    "我们可以根据 :numref:`fig_correlation` 构建输入张量`X`和核张量`K`，来[**验证上述二维互相关运算实现**]的输出结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f932dcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.272936Z",
     "iopub.status.busy": "2023-08-18T07:02:39.272409Z",
     "iopub.status.idle": "2023-08-18T07:02:39.281813Z",
     "shell.execute_reply": "2023-08-18T07:02:39.281016Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8bbf0",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "A convolutional layer cross-correlates the input and kernel\n",
    "and adds a scalar bias to produce an output.\n",
    "The two parameters of a convolutional layer\n",
    "are the kernel and the scalar bias.\n",
    "When training models based on convolutional layers,\n",
    "we typically initialize the kernels randomly,\n",
    "just as we would with a fully connected layer.\n",
    "\n",
    "We are now ready to [**implement a two-dimensional convolutional layer**]\n",
    "based on the `corr2d` function defined above.\n",
    "In the `__init__` constructor method,\n",
    "we declare `weight` and `bias` as the two model parameters.\n",
    "The forward propagation method\n",
    "calls the `corr2d` function and adds the bias.\n",
    "\n",
    "## 卷积层\n",
    "\n",
    "卷积层通过对输入和核进行互相关运算，并添加标量偏置来产生输出。卷积层的两个核心参数是核张量和标量偏置。在训练基于卷积层的模型时，我们通常采用与全连接层相同的策略——随机初始化卷积核参数。\n",
    "\n",
    "现在基于前面实现的`corr2d`函数[**构建二维卷积层**]。在`__init__`构造函数中：\n",
    "1. 声明`weight`和`bias`作为模型参数\n",
    "2. 前向传播方法中：\n",
    "   - 调用`corr2d`函数进行互相关运算\n",
    "   - 添加偏置项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0909a7f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.285293Z",
     "iopub.status.busy": "2023-08-18T07:02:39.284618Z",
     "iopub.status.idle": "2023-08-18T07:02:39.289470Z",
     "shell.execute_reply": "2023-08-18T07:02:39.288666Z"
    },
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8bc514",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "In\n",
    "$h \\times w$ convolution\n",
    "or an $h \\times w$ convolution kernel,\n",
    "the height and width of the convolution kernel are $h$ and $w$, respectively.\n",
    "We also refer to\n",
    "a convolutional layer with an $h \\times w$\n",
    "convolution kernel simply as an $h \\times w$ convolutional layer.\n",
    "\n",
    "\n",
    "## Object Edge Detection in Images\n",
    "\n",
    "Let's take a moment to parse [**a simple application of a convolutional layer:\n",
    "detecting the edge of an object in an image**]\n",
    "by finding the location of the pixel change.\n",
    "First, we construct an \"image\" of $6\\times 8$ pixels.\n",
    "The middle four columns are black ($0$) and the rest are white ($1$).\n",
    "\n",
    "## 图像中物体的边缘检测\n",
    "\n",
    "我们通过定位像素值变化的位置，来分析[**卷积层的一个简单应用：检测图像中物体的边缘**]。首先构造一个$6 \\times 8$像素的\"图像\"，中间四列为黑色（$0$），其余像素为白色（$1$）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8202c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.292802Z",
     "iopub.status.busy": "2023-08-18T07:02:39.292136Z",
     "iopub.status.idle": "2023-08-18T07:02:39.299029Z",
     "shell.execute_reply": "2023-08-18T07:02:39.298178Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b24ae5",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "Next, we construct a kernel `K` with a height of 1 and a width of 2.\n",
    "When we perform the cross-correlation operation with the input,\n",
    "if the horizontally adjacent elements are the same,\n",
    "the output is 0. Otherwise, the output is nonzero.\n",
    "Note that this kernel is a special case of a finite difference operator. At location $(i,j)$ it computes $x_{i,j} - x_{(i+1),j}$, i.e., it computes the difference between the values of horizontally adjacent pixels. This is a discrete approximation of the first derivative in the horizontal direction. After all, for a function $f(i,j)$ its derivative $-\\partial_i f(i,j) = \\lim_{\\epsilon \\to 0} \\frac{f(i,j) - f(i+\\epsilon,j)}{\\epsilon}$. Let's see how this works in practice.\n",
    "\n",
    "接下来构造一个高度为1、宽度为2的卷积核`K`。当对输入数据执行互相关运算时：\n",
    "- 若水平相邻元素相同，输出结果为0\n",
    "- 若存在差异，则输出非零值\n",
    "\n",
    "该卷积核实际上是有限差分算子的特例。在位置$(i,j)$处，它计算$x_{i,j} - x_{(i+1),j}$，即水平相邻像素值的差分。这本质上是水平方向一阶导数的离散近似。具体来说，对于函数$f(i,j)$，其导数可表示为：\n",
    "$$-\\partial_i f(i,j) = \\lim_{\\epsilon \\to 0} \\frac{f(i,j) - f(i+\\epsilon,j)}{\\epsilon}$$\n",
    "\n",
    "这种差分运算在实际应用中能够有效捕捉图像的水平边缘特征。我们通过下面的实例演示其工作原理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a67ee2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.302410Z",
     "iopub.status.busy": "2023-08-18T07:02:39.301879Z",
     "iopub.status.idle": "2023-08-18T07:02:39.305888Z",
     "shell.execute_reply": "2023-08-18T07:02:39.305083Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ae641",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "We are ready to perform the cross-correlation operation\n",
    "with arguments `X` (our input) and `K` (our kernel).\n",
    "As you can see, [**we detect $1$ for the edge from white to black\n",
    "and $-1$ for the edge from black to white.**]\n",
    "All other outputs take value $0$.\n",
    "\n",
    "通过输入张量`X`和核张量`K`的互相关运算，我们观察到：\n",
    "- **从白到黑的边缘**检测结果为$1$\n",
    "- **从黑到白的边缘**检测结果为$-1$\n",
    "- 其他所有位置的输出值均为$0$\n",
    "\n",
    "这种特征表明，我们设计的卷积核能够有效识别图像中物体边缘的位置和方向变化。正值边缘对应像素值从高到低的过渡(白→黑)，负值边缘则对应相反的过渡方向(黑→白)。这种微分特性使卷积层成为提取图像空间特征的强大工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9b02ed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.309118Z",
     "iopub.status.busy": "2023-08-18T07:02:39.308609Z",
     "iopub.status.idle": "2023-08-18T07:02:39.316193Z",
     "shell.execute_reply": "2023-08-18T07:02:39.315382Z"
    },
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a709f69",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "We can now apply the kernel to the transposed image.\n",
    "As expected, it vanishes. [**The kernel `K` only detects vertical edges.**]\n",
    "\n",
    "当我们将该卷积核应用于转置后的图像时，[**检测结果完全消失**]，这验证了`K`卷积核的专一性——它只能检测垂直边缘。这种特性源于卷积核的定向设计，其权重分布决定了它对特定方向的特征敏感度。在实际应用中，检测不同方向的边缘需要设计相应取向的卷积核，这也是深度学习模型中多个卷积层组合使用的原因之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3036f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.319798Z",
     "iopub.status.busy": "2023-08-18T07:02:39.319235Z",
     "iopub.status.idle": "2023-08-18T07:02:39.326588Z",
     "shell.execute_reply": "2023-08-18T07:02:39.325779Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794cf42",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## Learning a Kernel\n",
    "\n",
    "Designing an edge detector by finite differences `[1, -1]` is neat\n",
    "if we know this is precisely what we are looking for.\n",
    "However, as we look at larger kernels,\n",
    "and consider successive layers of convolutions,\n",
    "it might be impossible to specify\n",
    "precisely what each filter should be doing manually.\n",
    "\n",
    "Now let's see whether we can [**learn the kernel that generated `Y` from `X`**]\n",
    "by looking at the input--output pairs only.\n",
    "We first construct a convolutional layer\n",
    "and initialize its kernel as a random tensor.\n",
    "Next, in each iteration, we will use the squared error\n",
    "to compare `Y` with the output of the convolutional layer.\n",
    "We can then calculate the gradient to update the kernel.\n",
    "For the sake of simplicity,\n",
    "in the following\n",
    "we use the built-in class\n",
    "for two-dimensional convolutional layers\n",
    "and ignore the bias.\n",
    "\n",
    "## 学习卷积核\n",
    "\n",
    "通过有限差分算子`[1, -1]`设计边缘检测器虽然直观，但仅限于特定场景。当面对更大尺寸的卷积核或多层卷积堆叠时，手动设计每个滤波器的功能变得不切实际。\n",
    "\n",
    "我们现在尝试[**仅通过输入-输出对`(X,Y)`来学习生成`Y`的卷积核**]。具体步骤：\n",
    "1. 构建卷积层，将其卷积核初始化为随机张量\n",
    "2. 在每次迭代中：\n",
    "   - 使用平方误差损失比较卷积层输出与目标`Y`\n",
    "   - 计算梯度并更新卷积核参数\n",
    "   \n",
    "为简化实现，我们直接使用PyTorch内置的二维卷积层类，并暂时忽略偏置项。这种方法展示了通过数据驱动的方式自动学习特征提取器的核心思想，为后续深度学习模型的参数优化奠定了基础。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a38241f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.330085Z",
     "iopub.status.busy": "2023-08-18T07:02:39.329530Z",
     "iopub.status.idle": "2023-08-18T07:02:39.400974Z",
     "shell.execute_reply": "2023-08-18T07:02:39.400107Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 15.728\n",
      "epoch 4, loss 4.840\n",
      "epoch 6, loss 1.714\n",
      "epoch 8, loss 0.657\n",
      "epoch 10, loss 0.261\n"
     ]
    }
   ],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (example, channel, height, width), where the batch\n",
    "# size (number of examples in the batch) and the number of channels are both 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2  # Learning rate\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # Update the kernel\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b596b33",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "Note that the error has dropped to a small value after 10 iterations. Now we will [**take a look at the kernel tensor we learned.**]\n",
    "\n",
    "注意到误差在经历过10次迭代后已经跌落到一个很小的值，现在我们[**看看学习到的卷积核长什么样子**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "846630e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:39.404520Z",
     "iopub.status.busy": "2023-08-18T07:02:39.403956Z",
     "iopub.status.idle": "2023-08-18T07:02:39.409965Z",
     "shell.execute_reply": "2023-08-18T07:02:39.409163Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0389, -0.9343]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e810a",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "Indeed, the learned kernel tensor is remarkably close\n",
    "to the kernel tensor `K` we defined earlier.\n",
    "\n",
    "## Cross-Correlation and Convolution\n",
    "\n",
    "Recall our observation from :numref:`sec_why-conv` of the correspondence\n",
    "between the cross-correlation and convolution operations.\n",
    "Here let's continue to consider two-dimensional convolutional layers.\n",
    "What if such layers\n",
    "perform strict convolution operations\n",
    "as defined in :eqref:`eq_2d-conv-discrete`\n",
    "instead of cross-correlations?\n",
    "In order to obtain the output of the strict *convolution* operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the *cross-correlation* operation with the input tensor.\n",
    "\n",
    "It is noteworthy that since kernels are learned from data in deep learning,\n",
    "the outputs of convolutional layers remain unaffected\n",
    "no matter such layers\n",
    "perform\n",
    "either the strict convolution operations\n",
    "or the cross-correlation operations.\n",
    "\n",
    "To illustrate this, suppose that a convolutional layer performs *cross-correlation* and learns the kernel in :numref:`fig_correlation`, which is here denoted as the matrix $\\mathbf{K}$.\n",
    "Assuming that other conditions remain unchanged,\n",
    "when this layer instead performs strict *convolution*,\n",
    "the learned kernel $\\mathbf{K}'$ will be the same as $\\mathbf{K}$\n",
    "after $\\mathbf{K}'$ is\n",
    "flipped both horizontally and vertically.\n",
    "That is to say,\n",
    "when the convolutional layer\n",
    "performs strict *convolution*\n",
    "for the input in :numref:`fig_correlation`\n",
    "and $\\mathbf{K}'$,\n",
    "the same output in :numref:`fig_correlation`\n",
    "(cross-correlation of the input and $\\mathbf{K}$)\n",
    "will be obtained.\n",
    "\n",
    "In keeping with standard terminology in deep learning literature,\n",
    "we will continue to refer to the cross-correlation operation\n",
    "as a convolution even though, strictly-speaking, it is slightly different.\n",
    "Furthermore,\n",
    "we use the term *element* to refer to\n",
    "an entry (or component) of any tensor representing a layer representation or a convolution kernel.\n",
    "\n",
    "\n",
    "## Feature Map and Receptive Field\n",
    "\n",
    "As described in :numref:`subsec_why-conv-channels`,\n",
    "the convolutional layer output in\n",
    ":numref:`fig_correlation`\n",
    "is sometimes called a *feature map*,\n",
    "as it can be regarded as\n",
    "the learned representations (features)\n",
    "in the spatial dimensions (e.g., width and height)\n",
    "to the subsequent layer.\n",
    "In CNNs,\n",
    "for any element $x$ of some layer,\n",
    "its *receptive field* refers to\n",
    "all the elements (from all the previous layers)\n",
    "that may affect the calculation of $x$\n",
    "during the forward propagation.\n",
    "Note that the receptive field\n",
    "may be larger than the actual size of the input.\n",
    "\n",
    "Let's continue to use :numref:`fig_correlation` to explain the receptive field.\n",
    "Given the $2 \\times 2$ convolution kernel,\n",
    "the receptive field of the shaded output element (of value $19$)\n",
    "is\n",
    "the four elements in the shaded portion of the input.\n",
    "Now let's denote the $2 \\times 2$\n",
    "output as $\\mathbf{Y}$\n",
    "and consider a deeper CNN\n",
    "with an additional $2 \\times 2$ convolutional layer that takes $\\mathbf{Y}$\n",
    "as its input, outputting\n",
    "a single element $z$.\n",
    "In this case,\n",
    "the receptive field of $z$\n",
    "on $\\mathbf{Y}$ includes all the four elements of $\\mathbf{Y}$,\n",
    "while\n",
    "the receptive field\n",
    "on the input includes all the nine input elements.\n",
    "Thus,\n",
    "when any element in a feature map\n",
    "needs a larger receptive field\n",
    "to detect input features over a broader area,\n",
    "we can build a deeper network.\n",
    "\n",
    "\n",
    "Receptive fields derive their name from neurophysiology.\n",
    "A series of experiments on a range of animals using different stimuli\n",
    ":cite:`Hubel.Wiesel.1959,Hubel.Wiesel.1962,Hubel.Wiesel.1968` explored the response of what is called the visual\n",
    "cortex on said stimuli. By and large they found that lower levels respond to edges and related\n",
    "shapes. Later on, :citet:`Field.1987` illustrated this effect on natural\n",
    "images with, what can only be called, convolutional kernels.\n",
    "We reprint a key figure in :numref:`field_visual` to illustrate the striking similarities.\n",
    "\n",
    "![Figure and caption taken from :citet:`Field.1987`: An example of coding with six different channels. (Left) Examples of the six types of sensor associated with each channel. (Right) Convolution of the image in (Middle) with the six sensors shown in (Left). The response of the individual sensors is determined by sampling these filtered images at a distance proportional to the size of the sensor (shown with dots). This diagram shows the response of only the even symmetric sensors.](../img/field-visual.png)\n",
    ":label:`field_visual`\n",
    "\n",
    "As it turns out, this relation even holds for the features computed by deeper layers of networks trained on image classification tasks, as demonstrated in, for example, :citet:`Kuzovkin.Vicente.Petton.ea.2018`. Suffice it to say, convolutions have proven to be an incredibly powerful tool for computer vision, both in biology and in code. As such, it is not surprising (in hindsight) that they heralded the recent success in deep learning.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The core computation required for a convolutional layer is a cross-correlation operation. We saw that a simple nested for-loop is all that is required to compute its value. If we have multiple input and multiple output channels, we are  performing a matrix--matrix operation between channels. As can be seen, the computation is straightforward and, most importantly, highly *local*. This affords significant hardware optimization and many recent results in computer vision are only possible because of that. After all, it means that chip designers can invest in fast computation rather than memory when it comes to optimizing for convolutions. While this may not lead to optimal designs for other applications, it does open the door to ubiquitous and affordable computer vision.\n",
    "\n",
    "In terms of convolutions themselves, they can be used for many purposes, for example detecting edges and lines, blurring images, or sharpening them. Most importantly, it is not necessary that the statistician (or engineer) invents suitable filters. Instead, we can simply *learn* them from data. This replaces feature engineering heuristics by evidence-based statistics. Lastly, and quite delightfully, these filters are not just advantageous for building deep networks but they also correspond to receptive fields and feature maps in the brain. This gives us confidence that we are on the right track.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Construct an image `X` with diagonal edges.\n",
    "    1. What happens if you apply the kernel `K` in this section to it?\n",
    "    1. What happens if you transpose `X`?\n",
    "    1. What happens if you transpose `K`?\n",
    "1. Design some kernels manually.\n",
    "    1. Given a directional vector $\\mathbf{v} = (v_1, v_2)$, derive an edge-detection kernel that detects\n",
    "       edges orthogonal to $\\mathbf{v}$, i.e., edges in the direction $(v_2, -v_1)$.\n",
    "    1. Derive a finite difference operator for the second derivative. What is the minimum\n",
    "       size of the convolutional kernel associated with it? Which structures in images respond most strongly to it?\n",
    "    1. How would you design a blur kernel? Why might you want to use such a kernel?\n",
    "    1. What is the minimum size of a kernel to obtain a derivative of order $d$?\n",
    "1. When you try to automatically find the gradient for the `Conv2D` class we created, what kind of error message do you see?\n",
    "1. How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel tensors?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f7a44",
   "metadata": {
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/66)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fe995",
   "metadata": {},
   "source": [
    "## 互相关与卷积运算\n",
    "\n",
    "回忆我们在 :numref:`sec_why-conv` 中观察到的互相关运算与严格卷积运算的对应关系。对于二维卷积层，如果执行严格卷积运算（定义见 :eqref:`eq_2d-conv-discrete`）而非互相关运算会发生什么？要获得严格卷积的输出，我们只需要将二维核张量同时进行水平和垂直翻转，然后对输入张量执行互相关运算。\n",
    "\n",
    "值得注意的是，在深度学习中核是从数据中学习得到的，因此无论卷积层执行严格卷积还是互相关运算，其输出结果都不会受到影响。\n",
    "\n",
    "举例说明：假设卷积层执行互相关运算并学习得到 :numref:`fig_correlation` 中的核矩阵$\\mathbf{K}$。在其他条件不变的情况下，当该层改为执行严格卷积时，学习得到的核$\\mathbf{K}'$在经过水平和垂直翻转后将会与$\\mathbf{K}$相同。也就是说，当卷积层对 :numref:`fig_correlation` 中的输入和$\\mathbf{K}'$执行严格卷积时，将得到与 :numref:`fig_correlation` 中（输入与$\\mathbf{K}$的互相关）相同的输出。\n",
    "\n",
    "根据深度学习文献中的标准术语，我们仍将互相关运算称为卷积（尽管严格来说略有不同）。此外，我们使用元素（element）来指代任何表示层表示或卷积核的张量中的条目（或组件）。\n",
    "\n",
    "## 特征图与感受野\n",
    "\n",
    "如 :numref:`subsec_why-conv-channels` 所述，:numref:`fig_correlation` 中的卷积层输出有时被称为特征图（feature map），因为它可以被视为传递到后续层的空间维度（例如宽度和高度）上的学习表示（特征）。在CNN中，对于某个层的任意元素$x$，其感受野（receptive field）是指在正向传播期间可能影响$x$计算的所有先前层元素。注意感受野可能大于输入的实际尺寸。\n",
    "\n",
    "继续使用 :numref:`fig_correlation` 解释感受野：给定$2 \\times 2$卷积核，阴影输出元素（值为$19$）的感受野是输入中阴影部分的四个元素。现在假设将$2 \\times 2$输出记为$\\mathbf{Y}$，并考虑一个更深的CNN，其中包含额外的$2 \\times 2$卷积层，该层以$\\mathbf{Y}$作为输入并输出单个元素$z$。此时，$z$在$\\mathbf{Y}$上的感受野包括$\\mathbf{Y}$的所有四个元素，而在输入上的感受野包括所有九个输入元素。因此，当特征图中的任何元素需要更大的感受野来检测更广阔区域的输入特征时，我们可以构建更深的网络。\n",
    "\n",
    "感受野的概念来源于神经生理学。通过在不同动物身上进行的一系列刺激实验 :cite:`Hubel.Wiesel.1959,Hubel.Wiesel.1962,Hubel.Wiesel.1968`，研究者探索了视觉皮层对这些刺激的反应。他们发现较低层级主要响应边缘和相关形状。后来，:citet:`Field.1987` 使用卷积核在自然图像上展示了这种效果。我们在 :numref:`field_visual` 中重印了关键图示来说明这种惊人的相似性。\n",
    "\n",
    "![图片及说明取自 :citet:`Field.1987`：使用六个不同通道编码的示例。（左）与每个通道关联的六种传感器示例。（右）将（中）图与（左）中的六个传感器进行卷积的结果。单个传感器的响应通过按传感器尺寸比例采样这些滤波后的图像获得（用点表示）。该图仅显示偶对称传感器的响应。](../img/field-visual.png)\n",
    ":label:`field_visual`\n",
    "\n",
    "事实证明，这种关系甚至适用于经过图像分类任务训练的深层网络所计算的特征，如 :citet:`Kuzovkin.Vicente.Petton.ea.2018` 所示。可以说，卷积已被证明是计算机视觉领域（无论是生物学还是代码层面）极其强大的工具。因此，卷积预示深度学习最近的重大成功也就不足为奇了。\n",
    "\n",
    "## 小结\n",
    "\n",
    "卷积层需要的核心计算是互相关运算。我们看到，只需要简单的嵌套for循环就可以计算其值。当有多个输入和输出通道时，我们执行的是通道间的矩阵-矩阵运算。这种计算不仅简单，更重要的是高度局部化。这使得硬件优化成为可能，近年来计算机视觉的许多成果都得益于此。这意味着芯片设计者可以将优化重点放在快速计算而非内存上。虽然这对其他应用可能不是最优设计，但它为普及和实现经济高效的计算机视觉打开了大门。\n",
    "\n",
    "就卷积本身而言，它们可用于许多目的，例如检测边缘和线条、模糊图像或锐化图像。最重要的是，统计学家（或工程师）不需要手动设计合适的滤波器。相反，我们可以直接从数据中学习它们。这用基于证据的统计取代了特征工程的启发式方法。最后，令人欣喜的是，这些滤波器不仅有利于构建深度网络，而且与大脑中的感受野和特征图相对应。这使我们确信自己走在正确的道路上。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 构造具有对角边缘的图像`X`：\n",
    "    1. 将本节中的核`K`应用于该图像会发生什么？\n",
    "    1. 转置`X`后会发生什么？\n",
    "    1. 转置`K`后会发生什么？\n",
    "2. 手动设计一些核：\n",
    "    1. 给定方向向量$\\mathbf{v} = (v_1, v_2)$，推导检测与$\\mathbf{v}$正交的边缘检测核（即在$(v_2, -v_1)$方向的边缘）\n",
    "    1. 推导二阶导数的有限差分算子。相关卷积核的最小尺寸是多少？图像的哪些结构对其响应最强？\n",
    "    1. 如何设计模糊核？为什么要使用这样的核？\n",
    "    1. 要获得d阶导数，核的最小尺寸是多少？\n",
    "3. 当尝试为我们创建的`Conv2D`类自动求梯度时，会看到什么样的错误信息？\n",
    "4. 如何通过改变输入和核张量将互相关运算表示为矩阵乘法？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
