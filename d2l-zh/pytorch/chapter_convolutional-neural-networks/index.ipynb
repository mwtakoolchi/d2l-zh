{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41672dce",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Convolutional Neural Networks\n",
    ":label:`chap_cnn`\n",
    "\n",
    "Image data is represented as a two-dimensional grid of pixels, be the image\n",
    "monochromatic or in color. Accordingly each pixel corresponds to one\n",
    "or multiple numerical values respectively. So far we have ignored this rich\n",
    "structure and treated images as vectors of numbers by *flattening* them, irrespective of the spatial relation between pixels. This\n",
    "deeply unsatisfying approach was necessary in order to feed the\n",
    "resulting one-dimensional vectors through a fully connected MLP.\n",
    "\n",
    "Because these networks are invariant to the order of the features, we\n",
    "could get similar results regardless of whether we preserve an order\n",
    "corresponding to the spatial structure of the pixels or if we permute\n",
    "the columns of our design matrix before fitting the MLP's parameters.\n",
    "Ideally, we would leverage our prior knowledge that nearby pixels\n",
    "are typically related to each other, to build efficient models for\n",
    "learning from image data.\n",
    "\n",
    "This chapter introduces *convolutional neural networks* (CNNs)\n",
    ":cite:`LeCun.Jackel.Bottou.ea.1995`, a powerful family of neural networks that\n",
    "are designed for precisely this purpose.\n",
    "CNN-based architectures are\n",
    "now ubiquitous in the field of computer vision.\n",
    "For instance, on the Imagnet collection\n",
    ":cite:`Deng.Dong.Socher.ea.2009` it was only the use of convolutional neural\n",
    "networks, in short Convnets, that provided significant performance\n",
    "improvements :cite:`Krizhevsky.Sutskever.Hinton.2012`.\n",
    "\n",
    "Modern CNNs, as they are called colloquially, owe their design to\n",
    "inspirations from biology, group theory, and a healthy dose of\n",
    "experimental tinkering.  In addition to their sample efficiency in\n",
    "achieving accurate models, CNNs tend to be computationally efficient,\n",
    "both because they require fewer parameters than fully connected\n",
    "architectures and because convolutions are easy to parallelize across\n",
    "GPU cores :cite:`Chetlur.Woolley.Vandermersch.ea.2014`.  Consequently, practitioners often\n",
    "apply CNNs whenever possible, and increasingly they have emerged as\n",
    "credible competitors even on tasks with a one-dimensional sequence\n",
    "structure, such as audio :cite:`Abdel-Hamid.Mohamed.Jiang.ea.2014`, text\n",
    ":cite:`Kalchbrenner.Grefenstette.Blunsom.2014`, and time series analysis\n",
    ":cite:`LeCun.Bengio.ea.1995`, where recurrent neural networks are\n",
    "conventionally used.  Some clever adaptations of CNNs have also\n",
    "brought them to bear on graph-structured data :cite:`Kipf.Welling.2016` and\n",
    "in recommender systems.\n",
    "\n",
    "First, we will dive more deeply into the motivation for convolutional\n",
    "neural networks. This is followed by a walk through the basic operations\n",
    "that comprise the backbone of all convolutional networks.\n",
    "These include the convolutional layers themselves,\n",
    "nitty-gritty details including padding and stride,\n",
    "the pooling layers used to aggregate information\n",
    "across adjacent spatial regions,\n",
    "the use of multiple channels  at each layer,\n",
    "and a careful discussion of the structure of modern architectures.\n",
    "We will conclude the chapter with a full working example of LeNet,\n",
    "the first convolutional network successfully deployed,\n",
    "long before the rise of modern deep learning.\n",
    "In the next chapter, we will dive into full implementations\n",
    "of some popular and comparatively recent CNN architectures\n",
    "whose designs represent most of the techniques\n",
    "commonly used by modern practitioners.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [why-conv](why-conv.ipynb)\n",
    " - [conv-layer](conv-layer.ipynb)\n",
    " - [padding-and-strides](padding-and-strides.ipynb)\n",
    " - [channels](channels.ipynb)\n",
    " - [pooling](pooling.ipynb)\n",
    " - [lenet](lenet.ipynb)\n",
    ":end_tab:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a566d",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    ":label:`chap_cnn`\n",
    "\n",
    "图像数据表示为二维像素网格，无论是单色还是彩色图像。相应地，每个像素对应一个或多个数值。到目前为止，我们忽略了这种丰富的结构，将图像视为展平后的数值向量，而不考虑像素间的空间关系。这种简单粗暴的处理方式是为了能将一维向量输入全连接多层感知机。\n",
    "\n",
    "由于这些网络对特征顺序不敏感，无论我们是保持与像素空间结构对应的顺序，还是在拟合MLP参数前打乱设计矩阵的列序，都可能得到相似的结果。理想情况下，我们应该利用\"邻近像素通常相关\"的先验知识，构建高效的图像学习模型。\n",
    "\n",
    "本章介绍卷积神经网络（CNNs）:cite:`LeCun.Jackel.Bottou.ea.1995`——专为此目的设计的强大神经网络家族。基于CNN的架构如今在计算机视觉领域无处不在。例如在ImageNet数据集:cite:`Deng.Dong.Socher.ea.2009`上，正是卷积网络（简称ConvNets）的使用带来了显著的性能提升:cite:`Krizhevsky.Sutskever.Hinton.2012`。\n",
    "\n",
    "现代CNN的设计灵感来自生物学、群论和大量实验探索。除了在构建精确模型方面具有样本效率，CNN的计算效率也很高：既因为所需参数少于全连接架构，也因卷积运算易于跨GPU核心并行化:cite:`Chetlur.Woolley.Vandermersch.ea.2014`。因此，实践者尽可能使用CNN，甚至在音频:cite:`Abdel-Hamid.Mohamed.Jiang.ea.2014`、文本:cite:`Kalchbrenner.Grefenstette.Blunsom.2014`和时间序列分析:cite:`LeCun.Bengio.ea.1995`等一维序列任务中，它们也逐渐成为循环神经网络的有力竞争者。CNN的巧妙改进也使其适用于图结构数据:cite:`Kipf.Welling.2016`和推荐系统。\n",
    "\n",
    "我们将首先深入探讨卷积网络的动机，然后逐步讲解构成所有卷积网络骨架的基本运算，包括：卷积层本身（含填充和步幅细节）、用于聚合相邻区域信息的池化层、各层的多通道使用，以及现代架构结构的深入讨论。本章最后将通过LeNet（现代深度学习兴起前首个成功部署的卷积网络）的完整案例收尾。下一章将完整实现一些较新的CNN架构，这些设计体现了现代实践者常用的主要技术。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [为什么需要卷积](why-conv.ipynb)\n",
    " - [卷积层](conv-layer.ipynb) \n",
    " - [填充与步幅](padding-and-strides.ipynb)\n",
    " - [多输入输出通道](channels.ipynb)\n",
    " - [池化层](pooling.ipynb)\n",
    " - [LeNet网络](lenet.ipynb)\n",
    ":end_tab:\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
