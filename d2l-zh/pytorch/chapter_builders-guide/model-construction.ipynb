{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e6bcdf",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Layers and Modules\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "When we first introduced neural networks,\n",
    "we focused on linear models with a single output.\n",
    "Here, the entire model consists of just a single neuron.\n",
    "Note that a single neuron\n",
    "(i) takes some set of inputs;\n",
    "(ii) generates a corresponding scalar output;\n",
    "and (iii) has a set of associated parameters that can be updated\n",
    "to optimize some objective function of interest.\n",
    "Then, once we started thinking about networks with multiple outputs,\n",
    "we leveraged vectorized arithmetic\n",
    "to characterize an entire layer of neurons.\n",
    "Just like individual neurons,\n",
    "layers (i) take a set of inputs,\n",
    "(ii) generate corresponding outputs,\n",
    "and (iii) are described by a set of tunable parameters.\n",
    "When we worked through softmax regression,\n",
    "a single layer was itself the model.\n",
    "However, even when we subsequently\n",
    "introduced MLPs,\n",
    "we could still think of the model as\n",
    "retaining this same basic structure.\n",
    "\n",
    "Interestingly, for MLPs,\n",
    "both the entire model and its constituent layers\n",
    "share this structure.\n",
    "The entire model takes in raw inputs (the features),\n",
    "generates outputs (the predictions),\n",
    "and possesses parameters\n",
    "(the combined parameters from all constituent layers).\n",
    "Likewise, each individual layer ingests inputs\n",
    "(supplied by the previous layer)\n",
    "generates outputs (the inputs to the subsequent layer),\n",
    "and possesses a set of tunable parameters that are updated\n",
    "according to the signal that flows backwards\n",
    "from the subsequent layer.\n",
    "\n",
    "\n",
    "While you might think that neurons, layers, and models\n",
    "give us enough abstractions to go about our business,\n",
    "it turns out that we often find it convenient\n",
    "to speak about components that are\n",
    "larger than an individual layer\n",
    "but smaller than the entire model.\n",
    "For example, the ResNet-152 architecture,\n",
    "which is wildly popular in computer vision,\n",
    "possesses hundreds of layers.\n",
    "These layers consist of repeating patterns of *groups of layers*. Implementing such a network one layer at a time can grow tedious.\n",
    "This concern is not just hypothetical---such\n",
    "design patterns are common in practice.\n",
    "The ResNet architecture mentioned above\n",
    "won the 2015 ImageNet and COCO computer vision competitions\n",
    "for both recognition and detection :cite:`He.Zhang.Ren.ea.2016`\n",
    "and remains a go-to architecture for many vision tasks.\n",
    "Similar architectures in which layers are arranged\n",
    "in various repeating patterns\n",
    "are now ubiquitous in other domains,\n",
    "including natural language processing and speech.\n",
    "\n",
    "To implement these complex networks,\n",
    "we introduce the concept of a neural network *module*.\n",
    "A module could describe a single layer,\n",
    "a component consisting of multiple layers,\n",
    "or the entire model itself!\n",
    "One benefit of working with the module abstraction\n",
    "is that they can be combined into larger artifacts,\n",
    "often recursively. This is illustrated in :numref:`fig_blocks`. By defining code to generate modules\n",
    "of arbitrary complexity on demand,\n",
    "we can write surprisingly compact code\n",
    "and still implement complex neural networks.\n",
    "\n",
    "![Multiple layers are combined into modules, forming repeating patterns of larger models.](../img/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "\n",
    "From a programming standpoint, a module is represented by a *class*.\n",
    "Any subclass of it must define a forward propagation method\n",
    "that transforms its input into output\n",
    "and must store any necessary parameters.\n",
    "Note that some modules do not require any parameters at all.\n",
    "Finally a module must possess a backpropagation method,\n",
    "for purposes of calculating gradients.\n",
    "Fortunately, due to some behind-the-scenes magic\n",
    "supplied by the auto differentiation\n",
    "(introduced in :numref:`sec_autograd`)\n",
    "when defining our own module,\n",
    "we only need to worry about parameters\n",
    "and the forward propagation method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc2f37",
   "metadata": {},
   "source": [
    "# 层与模块\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "当我们首次介绍神经网络时，\n",
    "重点关注的是单输出的线性模型。\n",
    "此时整个模型仅由单个神经元组成。\n",
    "注意单个神经元：\n",
    "(i) 接收一组输入；\n",
    "(ii) 生成对应的标量输出；\n",
    "(iii) 拥有可更新参数集用于优化目标函数。\n",
    "\n",
    "当我们开始考虑具有多个输出的网络时，\n",
    "利用向量化运算描述整个神经元层。\n",
    "与单个神经元类似，\n",
    "层：\n",
    "(i) 接收一组输入；\n",
    "(ii) 生成对应输出；\n",
    "(iii) 由一组可调参数描述。\n",
    "在实现softmax回归时，\n",
    "单个层就构成了整个模型。\n",
    "即使后续引入多层感知机(MLP)时，\n",
    "模型仍保持这种基本结构。\n",
    "\n",
    "有趣的是，对于MLP，\n",
    "整个模型及其组成层共享相同结构。\n",
    "整个模型接收原始输入(特征)，\n",
    "生成输出(预测)，\n",
    "并拥有参数(各层参数集合)。\n",
    "每个独立层接收前层输入，\n",
    "生成后层输入，\n",
    "并拥有根据反向传播信号更新的可调参数集。\n",
    "\n",
    "虽然神经元、层和模型提供了足够的抽象，\n",
    "实践中常需要讨论比单层大、比整模小的组件。\n",
    "例如计算机视觉中广泛使用的ResNet-152架构，\n",
    "包含数百个层，\n",
    "这些层由重复的*层组模式*构成。\n",
    "逐层实现这样的网络会非常繁琐。\n",
    "ResNet架构在2015年ImageNet和COCO竞赛中\n",
    "斩获识别与检测双冠:cite:`He.Zhang.Ren.ea.2016`，\n",
    "至今仍是视觉任务的标杆方案。\n",
    "类似层重复模式架构也普遍存在于\n",
    "自然语言处理、语音等领域。\n",
    "\n",
    "为实现复杂网络，\n",
    "引入神经网络*模块*概念。\n",
    "模块可以是单层、多层组件或整个模型！\n",
    "模块化抽象的优点是能递归组合成更大结构，\n",
    "如 :numref:`fig_blocks` 所示。\n",
    "通过定义按需生成任意复杂度模块的代码，\n",
    "可用紧凑代码实现复杂神经网络。\n",
    "\n",
    "![多层组合成模块，形成更大模型的重复模式](../img/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "从编程角度看，\n",
    "模块由*类*表示。\n",
    "其子类必须定义：\n",
    "1. 将输入转为输出的前向传播方法\n",
    "2. 存储必要参数(部分模块可能无参)\n",
    "3. 反向传播方法(用于梯度计算)\n",
    "\n",
    "得益于自动微分(见 :numref:`sec_autograd`)的底层魔法，\n",
    "自定义模块时只需关注参数和前向传播方法，\n",
    "反向传播可自动处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e18055d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:07.018367Z",
     "iopub.status.busy": "2023-08-18T07:12:07.017538Z",
     "iopub.status.idle": "2023-08-18T07:12:09.132747Z",
     "shell.execute_reply": "2023-08-18T07:12:09.131739Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390b1d2",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "[**To begin, we revisit the code\n",
    "that we used to implement MLPs**]\n",
    "(:numref:`sec_mlp`).\n",
    "The following code generates a network\n",
    "with one fully connected hidden layer\n",
    "with 256 units and ReLU activation,\n",
    "followed by a fully connected output layer\n",
    "with ten units (no activation function).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e36b4",
   "metadata": {},
   "source": [
    "[**首先我们回顾一下多层感知机(MLP)的实现代码**]\n",
    "(:numref:`sec_mlp`)。\n",
    "以下代码生成一个网络结构：\n",
    "包含256个单元并使用ReLU激活函数的全连接隐藏层，\n",
    "后接一个10个单元且无激活函数的全连接输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6417a9e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.137561Z",
     "iopub.status.busy": "2023-08-18T07:12:09.136850Z",
     "iopub.status.idle": "2023-08-18T07:12:09.169150Z",
     "shell.execute_reply": "2023-08-18T07:12:09.168345Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5a511",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In this example, we constructed\n",
    "our model by instantiating an `nn.Sequential`, with layers in the order\n",
    "that they should be executed passed as arguments.\n",
    "In short, (**`nn.Sequential` defines a special kind of `Module`**),\n",
    "the class that presents a module in PyTorch.\n",
    "It maintains an ordered list of constituent `Module`s.\n",
    "Note that each of the two fully connected layers is an instance of the `Linear` class\n",
    "which is itself a subclass of `Module`.\n",
    "The forward propagation (`forward`) method is also remarkably simple:\n",
    "it chains each module in the list together,\n",
    "passing the output of each as input to the next.\n",
    "Note that until now, we have been invoking our models\n",
    "via the construction `net(X)` to obtain their outputs.\n",
    "This is actually just shorthand for `net.__call__(X)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be38ea",
   "metadata": {},
   "source": [
    "在这个例子中，我们通过实例化`nn.Sequential`来构建模型，\n",
    "按执行顺序传入各层作为参数。\n",
    "简而言之，( **`nn.Sequential`定义了一种特殊的`Module`** )，\n",
    "这是PyTorch中表示模块的基类。\n",
    "它维护着一个构成模块(Module)的有序列表。\n",
    "注意，两个全连接层都是`Linear`类的实例，\n",
    "而`Linear`类本身是`Module`的子类。\n",
    "前向传播(`forward`)方法也非常简单：\n",
    "将列表中的模块按顺序链式连接，\n",
    "将每个模块的输出作为下一个模块的输入。\n",
    "\n",
    "需要特别说明的是，\n",
    "我们之前一直通过`net(X)`的方式调用模型获取输出。\n",
    "这实际上是`net.__call__(X)`的语法糖。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8beb6c9",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## [**A Custom Module**]\n",
    "\n",
    "Perhaps the easiest way to develop intuition\n",
    "about how a module works\n",
    "is to implement one ourselves.\n",
    "Before we do that,\n",
    "we briefly summarize the basic functionality\n",
    "that each module must provide:\n",
    "\n",
    "\n",
    "1. Ingest input data as arguments to its forward propagation method.\n",
    "1. Generate an output by having the forward propagation method return a value. Note that the output may have a different shape from the input. For example, the first fully connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
    "1. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation method. Typically this happens automatically.\n",
    "1. Store and provide access to those parameters necessary\n",
    "   for executing the forward propagation computation.\n",
    "1. Initialize model parameters as needed.\n",
    "\n",
    "\n",
    "In the following snippet,\n",
    "we code up a module from scratch\n",
    "corresponding to an MLP\n",
    "with one hidden layer with 256 hidden units,\n",
    "and a 10-dimensional output layer.\n",
    "Note that the `MLP` class below inherits the class that represents a module.\n",
    "We will heavily rely on the parent class's methods,\n",
    "supplying only our own constructor (the `__init__` method in Python) and the forward propagation method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801c32d",
   "metadata": {},
   "source": [
    "## [**自定义模块**]\n",
    "\n",
    "要深入理解模块的工作原理，\n",
    "最直接的方式是自己实现一个。\n",
    "在此之前，我们先简要总结模块必须提供的基本功能：\n",
    "\n",
    "1. **输入处理**：通过前向传播方法接收输入数据\n",
    "2. **输出生成**：前向传播方法返回输出值(输出形状可能与输入不同，如示例中首个全连接层可接收任意维度输入但输出256维)\n",
    "3. **梯度计算**：通过反向传播方法自动计算输出相对于输入的梯度(通常自动完成)\n",
    "4. **参数管理**：存储并提供执行前向传播所需的参数访问\n",
    "5. **参数初始化**：按需初始化模型参数\n",
    "\n",
    "以下代码实现了一个自定义MLP模块，\n",
    "包含具有256个隐藏单元的隐藏层和10维输出层。\n",
    "注意`MLP`类继承自表示模块的基类，\n",
    "我们主要复用父类方法，\n",
    "仅需实现构造函数(`__init__`)和前向传播方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e937b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.172901Z",
     "iopub.status.busy": "2023-08-18T07:12:09.172537Z",
     "iopub.status.idle": "2023-08-18T07:12:09.178398Z",
     "shell.execute_reply": "2023-08-18T07:12:09.177233Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c93ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e8989",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "Let's first focus on the forward propagation method.\n",
    "Note that it takes `X` as input,\n",
    "calculates the hidden representation\n",
    "with the activation function applied,\n",
    "and outputs its logits.\n",
    "In this `MLP` implementation,\n",
    "both layers are instance variables.\n",
    "To see why this is reasonable, imagine\n",
    "instantiating two MLPs, `net1` and `net2`,\n",
    "and training them on different data.\n",
    "Naturally, we would expect them\n",
    "to represent two different learned models.\n",
    "\n",
    "We [**instantiate the MLP's layers**]\n",
    "in the constructor\n",
    "(**and subsequently invoke these layers**)\n",
    "on each call to the forward propagation method.\n",
    "Note a few key details.\n",
    "First, our customized `__init__` method\n",
    "invokes the parent class's `__init__` method\n",
    "via `super().__init__()`\n",
    "sparing us the pain of restating\n",
    "boilerplate code applicable to most modules.\n",
    "We then instantiate our two fully connected layers,\n",
    "assigning them to `self.hidden` and `self.out`.\n",
    "Note that unless we implement a new layer,\n",
    "we need not worry about the backpropagation method\n",
    "or parameter initialization.\n",
    "The system will generate these methods automatically.\n",
    "Let's try this out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c045b3",
   "metadata": {},
   "source": [
    "让我们首先关注前向传播方法的实现。\n",
    "该方法接收输入`X`，\n",
    "计算经过激活函数处理的隐藏表示，\n",
    "并输出其logits(未归一化的预测结果)。\n",
    "在此MLP实现中，\n",
    "两个层都定义为实例变量。\n",
    "这样设计的原因在于，\n",
    "假设我们实例化两个MLP网络`net1`和`net2`，\n",
    "并在不同数据上训练它们时，\n",
    "这两个网络应能学习到不同的模型参数。\n",
    "\n",
    "我们通过以下方式[**在构造函数中实例化MLP的层**]，\n",
    "并在每次前向传播时[**调用这些层**]。\n",
    "注意几个关键细节：\n",
    "1. 自定义的`__init__`方法通过`super().__init__()`调用父类构造函数，\n",
    "   避免重复编写适用于大多数模块的样板代码\n",
    "2. 实例化两个全连接层`self.hidden`和`self.out`\n",
    "3. 由于没有实现新层类型，\n",
    "   无需手动编写反向传播方法或参数初始化逻辑，\n",
    "   系统会自动生成这些方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fab23d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.181983Z",
     "iopub.status.busy": "2023-08-18T07:12:09.181698Z",
     "iopub.status.idle": "2023-08-18T07:12:09.188842Z",
     "shell.execute_reply": "2023-08-18T07:12:09.188082Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a994dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net = MyMLP()\n",
    "my_net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a823f6",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "A key virtue of the module abstraction is its versatility.\n",
    "We can subclass a module to create layers\n",
    "(such as the fully connected layer class),\n",
    "entire models (such as the `MLP` class above),\n",
    "or various components of intermediate complexity.\n",
    "We exploit this versatility\n",
    "throughout the coming chapters,\n",
    "such as when addressing\n",
    "convolutional neural networks.\n",
    "\n",
    "\n",
    "## [**The Sequential Module**]\n",
    ":label:`subsec_model-construction-sequential`\n",
    "\n",
    "We can now take a closer look\n",
    "at how the `Sequential` class works.\n",
    "Recall that `Sequential` was designed\n",
    "to daisy-chain other modules together.\n",
    "To build our own simplified `MySequential`,\n",
    "we just need to define two key methods:\n",
    "\n",
    "1. A method for appending modules one by one to a list.\n",
    "1. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended.\n",
    "\n",
    "The following `MySequential` class delivers the same\n",
    "functionality of the default `Sequential` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a8481",
   "metadata": {},
   "source": [
    "## [**顺序模块**]\n",
    ":label:`subsec_model-construction-sequential`\n",
    "\n",
    "模块抽象的核心优势在于其灵活性。\n",
    "我们可以通过继承模块类来创建：\n",
    "- 基础层（如全连接层）\n",
    "- 完整模型（如前述的`MLP`类）\n",
    "- 中等复杂度的组件\n",
    "\n",
    "这种灵活性在后续章节（如卷积神经网络）中将得到充分体现。\n",
    "\n",
    "### 自定义顺序模块实现\n",
    "要实现简化的`MySequential`类，\n",
    "需定义两个核心方法：\n",
    "1. **模块追加方法**：将模块逐个添加到列表\n",
    "2. **前向传播方法**：按添加顺序对输入进行链式处理\n",
    "\n",
    "以下是实现标准`Sequential`相同功能的代码示例：\n",
    "```python\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules_list = nn.ModuleList(args)  # 存储模块的容器\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.modules_list:  # 按添加顺序执行模块\n",
    "            X = module(X)\n",
    "        return X\n",
    "```\n",
    "关键实现细节说明：\n",
    "1. 使用`nn.ModuleList`容器确保参数正确注册\n",
    "2. 自动继承父类的参数管理功能\n",
    "3. 前向传播严格按模块添加顺序执行\n",
    "4. 支持动态添加模块（通过`add_module`方法扩展）\n",
    "\n",
    "使用示例：\n",
    "```python\n",
    "# 构建与nn.Sequential等效的模型\n",
    "model = MySequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "```\n",
    "该实现保留了PyTorch自动微分系统的兼容性，\n",
    "可通过相同方式调用`model(X)`进行前向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2468096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.193167Z",
     "iopub.status.busy": "2023-08-18T07:12:09.192676Z",
     "iopub.status.idle": "2023-08-18T07:12:09.198154Z",
     "shell.execute_reply": "2023-08-18T07:12:09.197007Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d93d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChiSequential(nn.Module):\n",
    "    def __init__(self, *args) -> None:\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc3ae5",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In the `__init__` method, we add every module\n",
    "by calling the `add_modules` method. These modules can be accessed by the `children` method at a later date.\n",
    "In this way the system knows the added modules,\n",
    "and it will properly initialize each module's parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b53e6",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "When our `MySequential`'s forward propagation method is invoked,\n",
    "each added module is executed\n",
    "in the order in which they were added.\n",
    "We can now reimplement an MLP\n",
    "using our `MySequential` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d5834a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c917eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.201622Z",
     "iopub.status.busy": "2023-08-18T07:12:09.201060Z",
     "iopub.status.idle": "2023-08-18T07:12:09.209374Z",
     "shell.execute_reply": "2023-08-18T07:12:09.208303Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70097fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_net = ChiSequential(\n",
    "    nn.Flatten(), nn.LazyLinear(256),nn.ReLU(),\n",
    "    nn.LazyLinear(10)\n",
    ")\n",
    "chi_net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68fe69b",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "Note that this use of `MySequential`\n",
    "is identical to the code we previously wrote\n",
    "for the `Sequential` class\n",
    "(as described in :numref:`sec_mlp`).\n",
    "\n",
    "\n",
    "## [**Executing Code in the Forward Propagation Method**]\n",
    "\n",
    "The `Sequential` class makes model construction easy,\n",
    "allowing us to assemble new architectures\n",
    "without having to define our own class.\n",
    "However, not all architectures are simple daisy chains.\n",
    "When greater flexibility is required,\n",
    "we will want to define our own blocks.\n",
    "For example, we might want to execute\n",
    "Python's control flow within the forward propagation method.\n",
    "Moreover, we might want to perform\n",
    "arbitrary mathematical operations,\n",
    "not simply relying on predefined neural network layers.\n",
    "\n",
    "You may have noticed that until now,\n",
    "all of the operations in our networks\n",
    "have acted upon our network's activations\n",
    "and its parameters.\n",
    "Sometimes, however, we might want to\n",
    "incorporate terms\n",
    "that are neither the result of previous layers\n",
    "nor updatable parameters.\n",
    "We call these *constant parameters*.\n",
    "Say for example that we want a layer\n",
    "that calculates the function\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$,\n",
    "where $\\mathbf{x}$ is the input, $\\mathbf{w}$ is our parameter,\n",
    "and $c$ is some specified constant\n",
    "that is not updated during optimization.\n",
    "So we implement a `FixedHiddenMLP` class as follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e76006",
   "metadata": {},
   "source": [
    "注意，这里使用的`MySequential`类与我们之前为`Sequential`类编写的代码完全相同（如 :numref:`sec_mlp` 所述）。\n",
    "\n",
    "## [**在前向传播方法中执行代码**]\n",
    "\n",
    "`Sequential`类使模型构建变得简单，允许我们在不定义自己的类的情况下组合新的架构。然而，并非所有架构都是简单的链式结构。当需要更大的灵活性时，我们将需要定义自己的块。例如，我们可能希望在前向传播方法中执行Python的控制流。此外，我们可能希望执行任意的数学运算，而不是仅仅依赖预定义的神经网络层。\n",
    "\n",
    "你可能已经注意到，到目前为止，我们网络中的所有操作都作用于网络的激活及其参数。但有时，我们可能希望引入既不是前几层结果也不是可更新参数的项。我们称这些为*固定参数*。例如，假设我们想要一个计算函数$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，其中$\\mathbf{x}$是输入，$\\mathbf{w}$是我们的参数，$c$是在优化期间不更新的指定常数。为此我们实现`FixedHiddenMLP`类如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d47827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.212894Z",
     "iopub.status.busy": "2023-08-18T07:12:09.212391Z",
     "iopub.status.idle": "2023-08-18T07:12:09.218820Z",
     "shell.execute_reply": "2023-08-18T07:12:09.217666Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55b7f4",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "In this model,\n",
    "we implement a hidden layer whose weights\n",
    "(`self.rand_weight`) are initialized randomly\n",
    "at instantiation and are thereafter constant.\n",
    "This weight is not a model parameter\n",
    "and thus it is never updated by backpropagation.\n",
    "The network then passes the output of this \"fixed\" layer\n",
    "through a fully connected layer.\n",
    "\n",
    "Note that before returning the output,\n",
    "our model did something unusual.\n",
    "We ran a while-loop, testing\n",
    "on the condition its $\\ell_1$ norm is larger than $1$,\n",
    "and dividing our output vector by $2$\n",
    "until it satisfied the condition.\n",
    "Finally, we returned the sum of the entries in `X`.\n",
    "To our knowledge, no standard neural network\n",
    "performs this operation.\n",
    "Note that this particular operation may not be useful\n",
    "in any real-world task.\n",
    "Our point is only to show you how to integrate\n",
    "arbitrary code into the flow of your\n",
    "neural network computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e7975",
   "metadata": {},
   "source": [
    "在该模型中，我们实现了一个隐藏层，其权重(`self.rand_weight`)在实例化时被随机初始化，之后保持固定。该权重不是模型参数，因此永远不会通过反向传播更新。网络随后将这个\"固定\"层的输出传递给全连接层。\n",
    "\n",
    "值得注意的是，在返回输出之前，我们的模型执行了一个特殊操作。我们运行了一个while循环，检测其$\\ell_1$范数是否大于1，如果条件满足就将输出向量除以2，直到满足条件为止。最后返回`X`中各元素的和。据我们所知，没有标准的神经网络会执行这种操作。需要说明的是，这个特定操作在实际任务中可能没有实用价值。我们的目的仅在于展示如何将任意代码集成到神经网络计算流程中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa6bf3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.223199Z",
     "iopub.status.busy": "2023-08-18T07:12:09.222901Z",
     "iopub.status.idle": "2023-08-18T07:12:09.231655Z",
     "shell.execute_reply": "2023-08-18T07:12:09.230832Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0724, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fd3f2",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "We can [**mix and match various\n",
    "ways of assembling modules together.**]\n",
    "In the following example, we nest modules\n",
    "in some creative ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96ade6",
   "metadata": {},
   "source": [
    "我们可以[**混合搭配各种方式组合模块**]。在下面的例子中，我们将以创造性的方式嵌套模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b670c349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:12:09.235299Z",
     "iopub.status.busy": "2023-08-18T07:12:09.234709Z",
     "iopub.status.idle": "2023-08-18T07:12:09.246189Z",
     "shell.execute_reply": "2023-08-18T07:12:09.245352Z"
    },
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0192, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3286eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0497, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class chiNestMLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                                nn.LazyLinear(64), nn.ReLU(),\n",
    "                                nn.LazyLinear(32), nn.ReLU()\n",
    "                            )\n",
    "        self.linear = nn.LazyLinear(64)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chi_chimera = nn.Sequential(chiNestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chi_chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00db95",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "## Summary\n",
    "\n",
    "Individual layers can be modules.\n",
    "Many layers can comprise a module.\n",
    "Many modules can comprise a module.\n",
    "\n",
    "A module can contain code.\n",
    "Modules take care of lots of housekeeping, including parameter initialization and backpropagation.\n",
    "Sequential concatenations of layers and modules are handled by the `Sequential` module.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. What kinds of problems will occur if you change `MySequential` to store modules in a Python list?\n",
    "1. Implement a module that takes two modules as an argument, say `net1` and `net2` and returns the concatenated output of both networks in the forward propagation. This is also called a *parallel module*.\n",
    "1. Assume that you want to concatenate multiple instances of the same network. Implement a factory function that generates multiple instances of the same module and build a larger network from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de998e",
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a54cc",
   "metadata": {},
   "source": [
    "## 摘要\n",
    "\n",
    "单个层可以是模块。\n",
    "多个层可以组成一个模块。\n",
    "多个模块可以组成更大的模块。\n",
    "\n",
    "模块可以包含代码。\n",
    "模块负责处理许多底层工作，包括参数初始化和反向传播。\n",
    "层和模块的顺序连接由`Sequential`模块处理。\n",
    "\n",
    "## 练习题\n",
    "\n",
    "1. 如果将`MySequential`改为使用Python列表存储模块会导致什么问题？\n",
    "2. 实现一个模块，接收两个模块`net1`和`net2`作为参数，在前向传播中返回两个网络输出的拼接结果（称为*并行模块*）。\n",
    "3. 假设需要拼接多个相同网络的实例，实现一个工厂函数来生成相同模块的多个实例，并构建更大的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2965b3",
   "metadata": {},
   "source": [
    "### 问题1深度分析（基于代码实现）\n",
    "原始ModuleList实现关键优势：\n",
    "```python\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(args)  # 关键注册\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "```\n",
    "改为Python列表会导致：\n",
    "1. **参数不可追踪**：优化器无法发现子模块参数\n",
    "2. **序列化黑洞**：`state_dict()`丢失子模块状态\n",
    "3. **设备隔离**：`.to(device)`无法传播到子模块\n",
    "4. **类型混淆**：无法识别嵌套Module结构\n",
    "\n",
    "### 增强版并行模块实现\n",
    "```python\n",
    "class Parallel(nn.Module):\n",
    "    def __init__(self, net1, net2, dim=1):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return torch.cat([self.net1(X), self.net2(X)], dim=self.dim)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Parallel({self.net1.__class__.__name__}, {self.net2.__class__.__name__}, dim={self.dim})\"\n",
    "\n",
    "# 验证示例\n",
    "net = Parallel(nn.Linear(20, 128), nn.Conv1d(20, 64, 3))\n",
    "print(net)\n",
    "# 输出：Parallel(Linear, Conv1d, dim=1)\n",
    "```\n",
    "\n",
    "### 工业级模块工厂实现\n",
    "```python\n",
    "from copy import deepcopy\n",
    "\n",
    "def module_factory(blueprint, num_copies, **kwargs):\n",
    "    \"\"\"动态生成参数隔离的模块副本\"\"\"\n",
    "    modules = []\n",
    "    for _ in range(num_copies):\n",
    "        new_mod = deepcopy(blueprint)\n",
    "        new_mod.reset_parameters()  # 假设模块实现参数重置方法\n",
    "        modules.append(new_mod)\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "# 使用示例\n",
    "base_layer = nn.Linear(256, 512)\n",
    "mega_net = module_factory(base_layer, 8)\n",
    "\n",
    "# 参数独立性验证\n",
    "for i, layer in enumerate(mega_net):\n",
    "    print(f\"Layer {i} weight ptr: {id(layer.weight)}\")\n",
    "# 输出不同内存地址\n",
    "```\n",
    "\n",
    "### 架构示意图更新\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph 工厂模式\n",
    "    A[原始模块] --> B[深拷贝]\n",
    "    B --> C[实例1]\n",
    "    B --> D[实例2]\n",
    "    C --> E[前向传播1]\n",
    "    D --> F[前向传播2]\n",
    "    end\n",
    "    \n",
    "    subgraph 并行计算\n",
    "    X[输入] --> Y[Parallel]\n",
    "    Y --> Z[网络A]\n",
    "    Y --> W[网络B]\n",
    "    Z --> M[输出A]\n",
    "    W --> N[输出B]\n",
    "    M --> CAT[拼接]\n",
    "    N --> CAT\n",
    "    end\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
