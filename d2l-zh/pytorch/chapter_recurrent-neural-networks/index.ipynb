{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd39adb",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Recurrent Neural Networks\n",
    ":label:`chap_rnn`\n",
    "\n",
    "Up until now, we have focused primarily on fixed-length data.\n",
    "When introducing linear and logistic regression\n",
    "in :numref:`chap_regression` and :numref:`chap_classification`\n",
    "and multilayer perceptrons in :numref:`chap_perceptrons`,\n",
    "we were happy to assume that each feature vector $\\mathbf{x}_i$\n",
    "consisted of a fixed number of components $x_1, \\dots, x_d$,\n",
    "where each numerical feature $x_j$\n",
    "corresponded to a particular attribute.\n",
    "These datasets are sometimes called *tabular*,\n",
    "because they can be arranged in tables,\n",
    "where each example $i$ gets its own row,\n",
    "and each attribute gets its own column.\n",
    "Crucially, with tabular data, we seldom\n",
    "assume any particular structure over the columns.\n",
    "\n",
    "Subsequently, in :numref:`chap_cnn`,\n",
    "we moved on to image data, where inputs consist\n",
    "of the raw pixel values at each coordinate in an image.\n",
    "Image data hardly fitted the bill\n",
    "of a protypical tabular dataset.\n",
    "There, we needed to call upon convolutional neural networks (CNNs)\n",
    "to handle the hierarchical structure and invariances.\n",
    "However, our data were still of fixed length.\n",
    "Every Fashion-MNIST image is represented\n",
    "as a $28 \\times 28$ grid of pixel values.\n",
    "Moreover, our goal was to develop a model\n",
    "that looked at just one image and then\n",
    "outputted a single prediction.\n",
    "But what should we do when faced with a\n",
    "sequence of images, as in a video,\n",
    "or when tasked with producing\n",
    "a sequentially structured prediction,\n",
    "as in the case of image captioning?\n",
    "\n",
    "A great many learning tasks require dealing with sequential data.\n",
    "Image captioning, speech synthesis, and music generation\n",
    "all require that models produce outputs consisting of sequences.\n",
    "In other domains, such as time series prediction,\n",
    "video analysis, and musical information retrieval,\n",
    "a model must learn from inputs that are sequences.\n",
    "These demands often arise simultaneously:\n",
    "tasks such as translating passages of text\n",
    "from one natural language to another,\n",
    "engaging in dialogue, or controlling a robot,\n",
    "demand that models both ingest and output\n",
    "sequentially structured data.\n",
    "\n",
    "\n",
    "Recurrent neural networks (RNNs) are deep learning models\n",
    "that capture the dynamics of sequences via\n",
    "*recurrent* connections, which can be thought of\n",
    "as cycles in the network of nodes.\n",
    "This might seem counterintuitive at first.\n",
    "After all, it is the feedforward nature of neural networks\n",
    "that makes the order of computation unambiguous.\n",
    "However, recurrent edges are defined in a precise way\n",
    "that ensures that no such ambiguity can arise.\n",
    "Recurrent neural networks are *unrolled* across time steps (or sequence steps),\n",
    "with the *same* underlying parameters applied at each step.\n",
    "While the standard connections are applied *synchronously*\n",
    "to propagate each layer's activations\n",
    "to the subsequent layer *at the same time step*,\n",
    "the recurrent connections are *dynamic*,\n",
    "passing information across adjacent time steps.\n",
    "As the unfolded view in :numref:`fig_unfolded-rnn` reveals,\n",
    "RNNs can be thought of as feedforward neural networks\n",
    "where each layer's parameters (both conventional and recurrent)\n",
    "are shared across time steps.\n",
    "\n",
    "\n",
    "![On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously.](../img/unfolded-rnn.svg)\n",
    ":label:`fig_unfolded-rnn`\n",
    "\n",
    "\n",
    "Like neural networks more broadly,\n",
    "RNNs have a long discipline-spanning history,\n",
    "originating as models of the brain popularized\n",
    "by cognitive scientists and subsequently adopted\n",
    "as practical modeling tools employed\n",
    "by the machine learning community.\n",
    "As we do for deep learning more broadly,\n",
    "in this book we adopt the machine learning perspective,\n",
    "focusing on RNNs as practical tools that rose\n",
    "to popularity in the 2010s owing to\n",
    "breakthrough results on such diverse tasks\n",
    "as handwriting recognition :cite:`graves2008novel`,\n",
    "machine translation :cite:`Sutskever.Vinyals.Le.2014`,\n",
    "and recognizing medical diagnoses :cite:`Lipton.Kale.2016`.\n",
    "We point the reader interested in more\n",
    "background material to a publicly available\n",
    "comprehensive review :cite:`Lipton.Berkowitz.Elkan.2015`.\n",
    "We also note that sequentiality is not unique to RNNs.\n",
    "For example, the CNNs that we already introduced\n",
    "can be adapted to handle data of varying length,\n",
    "e.g., images of varying resolution.\n",
    "Moreover, RNNs have recently ceded considerable\n",
    "market share to Transformer models,\n",
    "which will be covered in :numref:`chap_attention-and-transformers`.\n",
    "However, RNNs rose to prominence as the default models\n",
    "for handling complex sequential structure in deep learning,\n",
    "and remain staple models for sequential modeling to this day.\n",
    "The stories of RNNs and of sequence modeling\n",
    "are inextricably linked, and this is as much\n",
    "a chapter about the ABCs of sequence modeling problems\n",
    "as it is a chapter about RNNs.\n",
    "\n",
    "\n",
    "One key insight paved the way for a revolution in sequence modeling.\n",
    "While the inputs and targets for many fundamental tasks in machine learning\n",
    "cannot easily be represented as fixed-length vectors,\n",
    "they can often nevertheless be represented as\n",
    "varying-length sequences of fixed-length vectors.\n",
    "For example, documents can be represented as sequences of words;\n",
    "medical records can often be represented as sequences of events\n",
    "(encounters, medications, procedures, lab tests, diagnoses);\n",
    "videos can be represented as varying-length sequences of still images.\n",
    "\n",
    "\n",
    "While sequence models have popped up in numerous application areas,\n",
    "basic research in the area has been driven predominantly\n",
    "by advances on core tasks in natural language processing.\n",
    "Thus, throughout this chapter, we will focus\n",
    "our exposition and examples on text data.\n",
    "If you get the hang of these examples,\n",
    "then applying the models to other data modalities\n",
    "should be relatively straightforward.\n",
    "In the next few sections, we introduce basic\n",
    "notation for sequences and some evaluation measures\n",
    "for assessing the quality of sequentially structured model outputs.\n",
    "After that, we discuss basic concepts of a language model\n",
    "and use this discussion to motivate our first RNN models.\n",
    "Finally, we describe the method for calculating gradients\n",
    "when backpropagating through RNNs and explore some challenges\n",
    "that are often encountered when training such networks,\n",
    "motivating the modern RNN architectures that will follow\n",
    "in :numref:`chap_modern_rnn`.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [sequence](sequence.ipynb)\n",
    " - [text-sequence](text-sequence.ipynb)\n",
    " - [language-model](language-model.ipynb)\n",
    " - [rnn](rnn.ipynb)\n",
    " - [rnn-scratch](rnn-scratch.ipynb)\n",
    " - [rnn-concise](rnn-concise.ipynb)\n",
    " - [bptt](bptt.ipynb)\n",
    ":end_tab:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e7589",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    ":label:`chap_rnn`\n",
    "\n",
    "到目前为止，我们主要关注固定长度的数据。在 :numref:`chap_regression` 和 :numref:`chap_classification` 中介绍线性和逻辑回归，以及在 :numref:`chap_perceptrons` 中介绍多层感知机时，我们假设每个特征向量 $\\mathbf{x}_i$ 由固定数量的分量 $x_1, \\dots, x_d$ 组成，其中每个数值特征 $x_j$ 对应特定属性。这类数据集有时被称为*表格数据*，因为它们可以排列成表格，每行对应一个样本 $i$，每列对应一个属性。关键在于，对于表格数据，我们很少假设列之间存在特定结构。\n",
    "\n",
    "随后在 :numref:`chap_cnn` 中，我们转向图像数据，其输入由图像每个坐标的原始像素值组成。图像数据显然不符合典型表格数据集的特征。为此，我们需要借助卷积神经网络（CNN）来处理层次化结构和不变性。然而，我们的数据仍然是固定长度的。每个 Fashion-MNIST 图像都表示为 $28 \\times 28$ 的像素值网格。此外，我们的目标是开发一个模型：观察单张图像后输出单个预测。但当面对图像序列（如视频）或需要生成序列化预测（如图像描述）时，我们该怎么做？\n",
    "\n",
    "许多学习任务需要处理序列数据。图像描述生成、语音合成和音乐生成都需要模型输出由序列组成的结果。在其他领域，如时间序列预测、视频分析和音乐信息检索，模型必须从序列输入中学习。这些需求常常同时出现：诸如将文本段落从一种自然语言翻译成另一种、进行对话或控制机器人等任务，要求模型既能处理又能输出序列化结构数据。\n",
    "\n",
    "循环神经网络（RNN）是通过*循环连接*捕捉序列动态特性的深度学习模型，这些连接可视为网络节点间的循环结构。乍看这可能违反直觉，毕竟神经网络的前馈特性确保了计算顺序的明确性。然而，循环边界的精确定义消除了这种歧义。循环神经网络在时间步（或序列步）上展开，每个步骤应用相同的底层参数。标准连接*同步*应用于同一时间步中将各层激活传播至后续层，而循环连接则是*动态*的，在相邻时间步间传递信息。如 :numref:`fig_unfolded-rnn` 展开视图所示，RNN 可视为参数（常规和循环）跨时间步共享的前馈神经网络。\n",
    "\n",
    "![左侧通过循环边表示递归连接，右侧展示RNN在时间步上的展开。循环边跨越相邻时间步，常规连接同步计算。](../img/unfolded-rnn.svg)\n",
    ":label:`fig_unfolded-rnn`\n",
    "\n",
    "与广义神经网络类似，RNN 具有跨学科发展历史，最初作为认知科学家推广的脑模型，后被机器学习界采纳为实用建模工具。如同深度学习整体发展，本书采用机器学习视角，聚焦于 RNN 作为 2010 年代因多样任务突破性成果而流行的实用工具，例如手写识别 :cite:`graves2008novel`、机器翻译 :cite:`Sutskever.Vinyals.Le.2014` 和医疗诊断识别 :cite:`Lipton.Kale.2016`。感兴趣的读者可参阅公开的全面综述 :cite:`Lipton.Berkowitz.Elkan.2015`。需注意序列性并非 RNN 独有，例如已介绍的 CNN 可适配处理变长数据（如不同分辨率图像）。此外，RNN 近年已让出相当市场份额给 Transformer 模型（详见 :numref:`chap_attention-and-transformers`），但 RNN 仍是深度学习处理复杂序列结构的经典模型。RNN 与序列建模的故事密不可分，本章既是 RNN 专题，也是序列建模基础问题的全面阐述。\n",
    "\n",
    "关键洞见推动了序列建模的革命：虽然许多机器学习基础任务的输入和输出难以表示为定长向量，但它们常可表示为定长向量的变长序列。例如，文档可表示为词序列；医疗记录可表示为事件序列（就诊、用药、手术、检验、诊断）；视频可表示为静态图像的变长序列。\n",
    "\n",
    "虽然序列模型已应用于众多领域，但该领域的基础研究主要由自然语言处理核心任务进展驱动。因此，本章将重点阐述文本数据案例。掌握这些案例后，将模型应用于其他数据模态应相对直接。后续章节将介绍序列基本表示法、序列化模型输出的质量评估指标，讨论语言模型基础概念并由此引出首个 RNN 模型，最后阐述 RNN 反向传播梯度计算方法及训练挑战，为 :numref:`chap_modern_rnn` 现代 RNN 架构奠定基础。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [序列](sequence.ipynb)\n",
    " - [文本序列](text-sequence.ipynb)\n",
    " - [语言模型](language-model.ipynb)\n",
    " - [循环神经网络](rnn.ipynb)\n",
    " - [从零实现RNN](rnn-scratch.ipynb)\n",
    " - [简洁实现RNN](rnn-concise.ipynb)\n",
    " - [通过时间反向传播](bptt.ipynb)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
