{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0931002b",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Convolutional Neural Networks\n",
    ":label:`chap_cnn`\n",
    "\n",
    "Image data is represented as a two-dimensional grid of pixels, be the image\n",
    "monochromatic or in color. Accordingly each pixel corresponds to one\n",
    "or multiple numerical values respectively. So far we have ignored this rich\n",
    "structure and treated images as vectors of numbers by *flattening* them, irrespective of the spatial relation between pixels. This\n",
    "deeply unsatisfying approach was necessary in order to feed the\n",
    "resulting one-dimensional vectors through a fully connected MLP.\n",
    "\n",
    "Because these networks are invariant to the order of the features, we\n",
    "could get similar results regardless of whether we preserve an order\n",
    "corresponding to the spatial structure of the pixels or if we permute\n",
    "the columns of our design matrix before fitting the MLP's parameters.\n",
    "Ideally, we would leverage our prior knowledge that nearby pixels\n",
    "are typically related to each other, to build efficient models for\n",
    "learning from image data.\n",
    "\n",
    "This chapter introduces *convolutional neural networks* (CNNs)\n",
    ":cite:`LeCun.Jackel.Bottou.ea.1995`, a powerful family of neural networks that\n",
    "are designed for precisely this purpose.\n",
    "CNN-based architectures are\n",
    "now ubiquitous in the field of computer vision.\n",
    "For instance, on the Imagnet collection\n",
    ":cite:`Deng.Dong.Socher.ea.2009` it was only the use of convolutional neural\n",
    "networks, in short Convnets, that provided significant performance\n",
    "improvements :cite:`Krizhevsky.Sutskever.Hinton.2012`.\n",
    "\n",
    "Modern CNNs, as they are called colloquially, owe their design to\n",
    "inspirations from biology, group theory, and a healthy dose of\n",
    "experimental tinkering.  In addition to their sample efficiency in\n",
    "achieving accurate models, CNNs tend to be computationally efficient,\n",
    "both because they require fewer parameters than fully connected\n",
    "architectures and because convolutions are easy to parallelize across\n",
    "GPU cores :cite:`Chetlur.Woolley.Vandermersch.ea.2014`.  Consequently, practitioners often\n",
    "apply CNNs whenever possible, and increasingly they have emerged as\n",
    "credible competitors even on tasks with a one-dimensional sequence\n",
    "structure, such as audio :cite:`Abdel-Hamid.Mohamed.Jiang.ea.2014`, text\n",
    ":cite:`Kalchbrenner.Grefenstette.Blunsom.2014`, and time series analysis\n",
    ":cite:`LeCun.Bengio.ea.1995`, where recurrent neural networks are\n",
    "conventionally used.  Some clever adaptations of CNNs have also\n",
    "brought them to bear on graph-structured data :cite:`Kipf.Welling.2016` and\n",
    "in recommender systems.\n",
    "\n",
    "First, we will dive more deeply into the motivation for convolutional\n",
    "neural networks. This is followed by a walk through the basic operations\n",
    "that comprise the backbone of all convolutional networks.\n",
    "These include the convolutional layers themselves,\n",
    "nitty-gritty details including padding and stride,\n",
    "the pooling layers used to aggregate information\n",
    "across adjacent spatial regions,\n",
    "the use of multiple channels  at each layer,\n",
    "and a careful discussion of the structure of modern architectures.\n",
    "We will conclude the chapter with a full working example of LeNet,\n",
    "the first convolutional network successfully deployed,\n",
    "long before the rise of modern deep learning.\n",
    "In the next chapter, we will dive into full implementations\n",
    "of some popular and comparatively recent CNN architectures\n",
    "whose designs represent most of the techniques\n",
    "commonly used by modern practitioners.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [why-conv](why-conv.ipynb)\n",
    " - [conv-layer](conv-layer.ipynb)\n",
    " - [padding-and-strides](padding-and-strides.ipynb)\n",
    " - [channels](channels.ipynb)\n",
    " - [pooling](pooling.ipynb)\n",
    " - [lenet](lenet.ipynb)\n",
    ":end_tab:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d203cf6",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    ":label:`chap_cnn`\n",
    "\n",
    "图像数据是以二维像素网格的形式呈现的，无论是黑白还是彩色图像。相应地，每个像素对应一个或多个数值。迄今为止，我们忽略了这种丰富的结构，通过将图像展平（*flattening*）为向量来处理，完全无视像素间的空间关系。这种不尽人意的做法是为了将生成的一维向量输入全连接多层感知机（MLP）而不得不采取的措施。\n",
    "\n",
    "由于这些网络对特征顺序具有不变性，无论我们是保持与像素空间结构对应的顺序，还是在拟合MLP参数前对设计矩阵的列进行置换，都能得到相似的结果。理想情况下，我们应该利用邻近像素通常相互关联这一先验知识，构建高效的图像数据学习模型。\n",
    "\n",
    "本章将介绍专为此目的设计的*卷积神经网络*（CNNs）:cite:`LeCun.Jackel.Bottou.ea.1995`。基于CNN的架构如今已广泛应用于计算机视觉领域。例如，在ImageNet数据集:cite:`Deng.Dong.Socher.ea.2009`上，正是卷积神经网络（简称ConvNets）的使用带来了显著的性能提升:cite:`Krizhevsky.Sutskever.Hinton.2012`。\n",
    "\n",
    "现代CNN的设计灵感源于生物学原理、群论思想以及大量的实验探索。除了在实现高精度模型时具有样本效率的优势外，CNN的计算效率也较高——这既得益于其相比全连接架构所需的参数更少，也归功于卷积运算在GPU核心上的易并行性:cite:`Chetlur.Woolley.Vandermersch.ea.2014`。因此，实践者只要条件允许就会优先选择CNN。如今，即使在音频:cite:`Abdel-Hamid.Mohamed.Jiang.ea.2014`、文本:cite:`Kalchbrenner.Grefenstette.Blunsom.2014`和时间序列分析:cite:`LeCun.Bengio.ea.1995`等传统上使用循环神经网络的一维序列任务中，CNN也已成为有力的竞争者。一些巧妙的改进方案还将CNN成功应用于图结构数据:cite:`Kipf.Welling.2016`和推荐系统。\n",
    "\n",
    "首先，我们将深入探讨卷积神经网络的设计动机。接着详细讲解构成所有卷积网络骨干的基本操作，包括：卷积层本身、填充（padding）与步幅（stride）等实现细节、用于聚合相邻空间区域信息的池化层、各层的多通道使用，以及对现代架构结构的深入讨论。本章最后将通过LeNet的完整实例——这是现代深度学习兴起之前首个成功部署的卷积网络——作为总结。下一章我们将深入解析若干相对近期且流行的CNN架构实现，这些设计体现了现代实践者最常使用的技术方案。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [为什么需要卷积](why-conv.ipynb)\n",
    " - [卷积层](conv-layer.ipynb)\n",
    " - [填充与步幅](padding-and-strides.ipynb)\n",
    " - [多输入多输出通道](channels.ipynb)\n",
    " - [池化层](pooling.ipynb)\n",
    " - [LeNet网络](lenet.ipynb)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
